# qlora-chinese-example
ä½¿ç”¨qloraå¯¹ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚

ä½¿ç”¨qloraå¯¹baichuan-7bè¿›è¡Œå¾®è°ƒï¼Œä»£ç æ›´åŠ ç®€æ´ï¼šhttps://github.com/taishan1994/baichuan-Qlora-Tuning

# ä¾èµ–

```python
numpy==1.24.2
pandas==2.0.0
nltk==3.7
transformers==4.30.0.dev0
accelerate==0.20.0.dev0
deepspeed==0.9.2
peft==0.4.0.dev0
datasets==2.12.0
evaluate==0.2.2
sentencepiece==0.1.97
scipy==1.10.1
icetk
cpm_kernels
mpi4py==3.1.4
```

# ç›®å½•ç»“æ„

```python
--output #è®­ç»ƒä¿å­˜loraæƒé‡
----chatglm
----alpaca
----bloom
--data
----msra
------instruct_data
--------train.json  #æŒ‡ä»¤æ•°æ®
--model_hub
----BELLE-7B-2M #bloomæƒé‡
----chatglm-6b  #chatGLMæƒé‡
----7Bï¼š#è‹±æ–‡LLaMAåŸå§‹æƒé‡
----7B-hfï¼š#è‹±æ–‡æƒé‡è½¬æ¢ä¸ºhugging faceæ ¼å¼æƒé‡
----chinese-llama-plus-lora-7bï¼š#ä¸­æ–‡llama-7bçš„loraæƒé‡
----chinese-alpaca-plus-lora-7bï¼š#ä¸­æ–‡alpaca-7bçš„loraæƒé‡
----chinese-alpaca-7bï¼š#åˆå¹¶loraåçš„æœ€ç»ˆçš„æ¨¡å‹
----tokenizer.modelï¼š#åŸå§‹llamaçš„7Bæ–‡ä»¶
----convert_llama_weights_to_hf.py  #llamaè½¬æ¢ä¸ºhugging faceæ ¼å¼
----merge_llama_with_chinese_lora.py  #åˆå¹¶loraåˆ°é¢„è®­ç»ƒæ¨¡å‹
--tools
----get_version.py  #è·å–pythonåŒ…ç‰ˆæœ¬
----get_used_gpus.py  #å¾ªç¯æ‰“å°ä½¿ç”¨çš„GPUæ˜¾å¡
--chat.py  # é—²èŠ
--qlora.py  # 4bitè®­ç»ƒ
--process.py  # æµ‹è¯•å¤„ç†æ•°æ®
```

# ChatGLM

ChatGLM-6Bä¸‹è½½åœ°å€ï¼š[æ¸…åå¤§å­¦äº‘ç›˜ (tsinghua.edu.cn)](https://cloud.tsinghua.edu.cn/d/fb9f16d6dc8f482596c2/) 

```python
python qlora.py --model_name="chatglm" --model_name_or_path="./model_hub/chatglm-6b" --trust_remote_code=True --dataset="msra" --source_max_len=128 --target_max_len=64 --do_train --save_total_limit=1 --padding_side="left" --per_device_train_batch_size=8 --do_eval --bits=4 --save_steps=10 --gradient_accumulation_steps=1 --learning_rate=1e-5 --output_dir="./output/chatglm/" --lora_r=8 --lora_alpha=32
```

# Alpaca

Facebookå®˜æ–¹å‘å¸ƒçš„[LLaMAæ¨¡å‹ç¦æ­¢å•†ç”¨](https://github.com/facebookresearch/llama)ï¼Œå¹¶ä¸”å®˜æ–¹æ²¡æœ‰æ­£å¼å¼€æºæ¨¡å‹æƒé‡ï¼ˆè™½ç„¶ç½‘ä¸Šå·²ç»æœ‰å¾ˆå¤šç¬¬ä¸‰æ–¹çš„ä¸‹è½½åœ°å€ï¼‰ã€‚ä¸ºäº†éµå¾ªç›¸åº”çš„è®¸å¯ï¼Œç›®å‰æš‚æ—¶æ— æ³•å‘å¸ƒå®Œæ•´çš„æ¨¡å‹æƒé‡ï¼Œæ•¬è¯·å„ä½ç†è§£ï¼ˆç›®å‰å›½å¤–ä¹Ÿæ˜¯ä¸€æ ·ï¼‰ã€‚è‡ªè¡Œæœç´¢ä¸‹è½½åœ°å€ã€‚

## è½¬æ¢æ­¥éª¤

- 1ã€ä¸‹è½½å¥½7Bã€[llama-lora](https://huggingface.co/ziqingyang/chinese-llama-plus-lora-7b)ã€[alpaca-lora](https://huggingface.co/ziqingyang/chinese-alpaca-plus-lora-7b)åˆ°model_hubä¸‹ã€‚è¿›å…¥åˆ°model_hubç›®å½•ä¸‹ã€‚
- 2ã€å°†llamaè½¬æ¢ä¸ºhugging faceæ”¯æŒçš„æ ¼å¼ï¼š`python convert_llama_weights_to_hf.py --input_dir ./ --model_size 7B --output_dir ./7B-hf`ã€‚å¦‚æœæŠ¥é”™ï¼š`If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0`åˆ™å¯ä»¥`pip install --upgrade protobuf==3.20.1`ï¼Œç„¶åï¼š`python convert_llama_weights_to_hf.py --input_dir ./ --model_size tokenizer_only --output_dir ./7B-hf`ã€‚æœ€ç»ˆæˆ‘ä»¬å¯ä»¥å¾—åˆ°7B-hfã€‚
- 3ã€åˆå¹¶loraåˆ°llamaä¸Šï¼š`python merge_llama_with_chinese_lora.py --base_model "./7B-hf" --lora_model "./chinese-llama-plus-lora-7b,chinese-alpaca-plus-lora-7b" --output_type "huggingface" --output_dir "./chinese-alpaca-7b" `ã€‚æœ€ç»ˆæˆ‘ä»¬å¯ä»¥å¾—åˆ°chinese-alpaca-7bã€‚

```python
python qlora.py --model_name="chinese_alpaca" --model_name_or_path="./model_hub/chinese-alpaca-7b" --trust_remote_code=False --dataset="msra" --source_max_len=128 --target_max_len=64 --do_train --save_total_limit=1 --padding_side="right" --per_device_train_batch_size=8 --do_eval --bits=4 --save_steps=10 --gradient_accumulation_steps=1 --learning_rate=1e-5 --output_dir="./output/alpaca/" --lora_r=8 --lora_alpha=32
```

# BLOOM

BELLE-7B-2Mä¸‹è½½åœ°å€ï¼š[BelleGroup/BELLE-7B-2M at main (huggingface.co)](https://huggingface.co/BelleGroup/BELLE-7B-2M/tree/main)

```python
python qlora.py --model_name="chinese_bloom" --model_name_or_path="./model_hub/BELLE-7B-2M" --trust_remote_code=False --dataset="msra" --source_max_len=128 --target_max_len=64 --do_train --save_total_limit=1 --padding_side="left" --per_device_train_batch_size=8 --do_eval --bits=4 --save_steps=10 --gradient_accumulation_steps=1 --learning_rate=1e-5 --output_dir="./output/bloom/" --lora_r=8 --lora_alpha=32
```

# é—²èŠ

```python
python chat.py --model_name "chatglm" --base_model "./model_hub/chatglm-6b" --tokenizer_path "./model_hub/chatglm-6b" --lora_model "./output/chatglm/adapter_model" --with_prompt --interactive
```

# è¡¥å……

- **æ€ä¹ˆè®­ç»ƒè‡ªå·±çš„æ•°æ®ï¼Ÿ** æ•°æ®æ ¼å¼ä¸ºï¼š

	```python
	{
	    "data": [
	        {"instruction":"", "input":"", "output":""},
	        {"instruction":"", "input":"", "output":""},
	        ...
	    ]
	}
	```

	ç„¶ååœ¨qlora.pyé‡Œé¢å®šä¹‰æ•°æ®çš„åœ°æ–¹åŠ ä¸Šè‡ªå·±çš„æ•°æ®é›†å³å¯ã€‚æœ€åè¿è¡ŒæŒ‡ä»¤çš„æ—¶å€™è‡ªå·±å®šä¹‰ç›¸å…³çš„å‚æ•°ã€‚

# å‚è€ƒ

> [liucongg/ChatGLM-Finetuning: åŸºäºChatGLM-6Bæ¨¡å‹ï¼Œè¿›è¡Œä¸‹æ¸¸å…·ä½“ä»»åŠ¡å¾®è°ƒï¼Œæ¶‰åŠFreezeã€Loraã€P-tuningç­‰ (github.com)](https://github.com/liucongg/ChatGLM-Finetuning)
>
> [THUDM/ChatGLM-6B: ChatGLM-6B: An Open Bilingual Dialogue Language Model | å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ (github.com)](https://github.com/THUDM/ChatGLM-6B/projects?query=is%3Aopen)
>
> [huggingface/peft: ğŸ¤— PEFT: State-of-the-art Parameter-Efficient Fine-Tuning. (github.com)](https://github.com/huggingface/peft)
>
> [ymcui/Chinese-LLaMA-Alpaca: ä¸­æ–‡LLaMA&Alpacaå¤§è¯­è¨€æ¨¡å‹+æœ¬åœ°CPU/GPUè®­ç»ƒéƒ¨ç½² (Chinese LLaMA & Alpaca LLMs) (github.com)](https://github.com/ymcui/Chinese-LLaMA-Alpaca)
>
> [LianjiaTech/BELLE: BELLE: Be Everyone's Large Language model Engineï¼ˆå¼€æºä¸­æ–‡å¯¹è¯å¤§æ¨¡å‹ï¼‰ (github.com)](https://github.com/LianjiaTech/BELLE/)
>
> [artidoro/qlora: QLoRA: Efficient Finetuning of Quantized LLMs (github.com)](https://github.com/artidoro/qlora)

**å“ªä½å¥½å¿ƒäººå£«å¡å¤šçš„è¯•è¯•çœ‹æœ€ç»ˆæ•ˆæœå§ï¼Œç§ŸAutoDLå…¥ä¸æ•·å‡ºå‘€**
